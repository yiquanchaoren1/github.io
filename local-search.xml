<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>gemm+im2col</title>
    <link href="/2024/07/01/gemm_im2col/"/>
    <url>/2024/07/01/gemm_im2col/</url>
    
    <content type="html"><![CDATA[<h2 id="gemm-im2col"><a href="#gemm-im2col" class="headerlink" title="gemm+im2col"></a>gemm+im2col</h2><h4 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h4><p><a href="https://blog.csdn.net/taoqick/article/details/129051936">高性能卷积计算：img2col 原理详解-CSDN博客</a></p><p><a href="https://seanwangjs.github.io/2022/04/28/im2col-programming.html">使用 C++ 实现 im2col 操作 - Fenrier Lab (seanwangjs.github.io)</a></p><h3 id="im2col"><a href="#im2col" class="headerlink" title="im2col"></a>im2col</h3><p>img2col 是一种实现卷积操作的加速计算策略。它能将卷积操作转化为 GEMM，从而最大化地缩短卷积计算的时间。</p><h4 id="为什么要将卷积操作转化为-GEMM-呢？"><a href="#为什么要将卷积操作转化为-GEMM-呢？" class="headerlink" title="为什么要将卷积操作转化为 GEMM 呢？"></a><strong>为什么要将卷积操作转化为 GEMM 呢？</strong></h4><p>1.因为线性代数领域已经有非常成熟的计算接口（BLAS，Fortran 语言实现）来高效地实现大型的矩阵乘法，几乎可以做到极限优化。</p><p>2.将卷积过程中用到的所有特征子矩阵整合成一个大型矩阵存放在连续的内存中，虽然增加了存储成本，但是减少了内存访问的次数，从而缩短了计算时间。</p><h4 id="img2col将卷积操作转化为GEMM过程"><a href="#img2col将卷积操作转化为GEMM过程" class="headerlink" title="img2col将卷积操作转化为GEMM过程"></a>img2col将卷积操作转化为GEMM过程</h4><p><img src="/../images/1719824785939.png" alt="1719824785939"></p><h5 id="1-Input-Features-Input-Matrix"><a href="#1-Input-Features-Input-Matrix" class="headerlink" title="1.Input Features &gt; Input Matrix"></a>1.Input Features &gt; Input Matrix</h5><p><img src="/../images/1719824926813.png" alt="1719824926813"></p><p>输入特征图有三个通道，用三个不同的颜色来表示</p><p>因为卷积核的大小为2乘2，当卷积核的滑动步长为1的时候，那么传统的直接卷积计算一共需要进行 4 次卷积核与对应特征子矩阵之间的点积运算。</p><p>现在把每一个子矩阵都排列成一个行向量，就得到了三个通道的特征图对应的三个矩阵（Input Matrix）</p><h5 id="2-Convolution-Kernel-Kernel-Matrix"><a href="#2-Convolution-Kernel-Kernel-Matrix" class="headerlink" title="2.Convolution Kernel &gt; Kernel Matrix"></a>2.Convolution Kernel &gt; Kernel Matrix</h5><p><img src="/../images/1719825159189.png" alt="1719825159189"></p><p>卷积核有两个，每一个也是三通道的，我们研究其中一个卷积核</p><p>将卷积核转换为列向量，然后再把每一个通道对应的 Kernel Matrix 堆叠成一个完整的 Kernel Matrix。</p><h5 id="3-Input-Matrix-Kernel-Matrix-Output-Matrix"><a href="#3-Input-Matrix-Kernel-Matrix-Output-Matrix" class="headerlink" title="3.Input Matrix * Kernel Matrix &#x3D; Output Matrix"></a>3.Input Matrix * Kernel Matrix &#x3D; Output Matrix</h5><p>调用GEMM接口，将两个矩阵进行乘积。然后将输出矩阵通过 col2img 函数就可以得到和卷积运算一样的输出特征图。</p><p><img src="/../images/1719825458677.png" alt="1719825458677"></p><h4 id="c-代码实现im2col"><a href="#c-代码实现im2col" class="headerlink" title="c++代码实现im2col"></a>c++代码实现im2col</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">im2col</span><span class="hljs-params">(<span class="hljs-type">const</span> <span class="hljs-type">float</span>* data_im, </span></span><br><span class="hljs-params"><span class="hljs-function">    <span class="hljs-type">const</span> <span class="hljs-type">int</span> im_c, </span></span><br><span class="hljs-params"><span class="hljs-function">    <span class="hljs-type">const</span> <span class="hljs-type">int</span> im_w, </span></span><br><span class="hljs-params"><span class="hljs-function">    <span class="hljs-type">const</span> <span class="hljs-type">int</span> im_h, </span></span><br><span class="hljs-params"><span class="hljs-function">    <span class="hljs-type">const</span> <span class="hljs-type">int</span> kw, </span></span><br><span class="hljs-params"><span class="hljs-function">    <span class="hljs-type">const</span> <span class="hljs-type">int</span> kh, </span></span><br><span class="hljs-params"><span class="hljs-function">    <span class="hljs-type">const</span> <span class="hljs-type">int</span> ph, </span></span><br><span class="hljs-params"><span class="hljs-function">    <span class="hljs-type">const</span> <span class="hljs-type">int</span> pw, </span></span><br><span class="hljs-params"><span class="hljs-function">    <span class="hljs-type">const</span> <span class="hljs-type">int</span> sh,</span></span><br><span class="hljs-params"><span class="hljs-function">    <span class="hljs-type">const</span> <span class="hljs-type">int</span> sw,</span></span><br><span class="hljs-params"><span class="hljs-function">    <span class="hljs-type">float</span>* data_col, </span></span><br><span class="hljs-params"><span class="hljs-function">    <span class="hljs-type">const</span> <span class="hljs-type">int</span> col_w, </span></span><br><span class="hljs-params"><span class="hljs-function">    <span class="hljs-type">const</span> <span class="hljs-type">int</span> col_h)</span> </span>&#123;<br><br>        <span class="hljs-comment">// win_w and win_h are the stop times of the kernel in the image.</span><br>        <span class="hljs-type">int</span> win_w = (im_w + <span class="hljs-number">2</span> * pw - kw + <span class="hljs-number">1</span>) / sw;<br>        <span class="hljs-type">int</span> win_h = (im_h + <span class="hljs-number">2</span> * ph - kh + <span class="hljs-number">1</span>) / sh;<br><br>        <br>        <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>; i&lt; col_h; i++) &#123;<br><br>            x = i % win_w;<br>            y = i / win_w;<br>            <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j = <span class="hljs-number">0</span>; j &lt; col_w; j++) &#123;<br>                <span class="hljs-type">int</span> c = j / (kw * kh);<br>                <span class="hljs-type">int</span> kj = j % kw;<br>                <span class="hljs-type">int</span> ki = j / kw;<br><br>                <span class="hljs-type">int</span> row = y * sh + ki;<br>                <span class="hljs-type">int</span> col = x * sw + kj;<br><br>                data_col[i * col_w + j] = <span class="hljs-built_in">get_data</span>(data_im, c, im_w, im_h, row, col, ph, pw);<br>            &#125;<br>        &#125;<br><br>    &#125;<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>hpc</category>
      
    </categories>
    
    
    <tags>
      
      <tag>hpc</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>cublas库 学习</title>
    <link href="/2024/07/01/cublas%E5%BA%93/"/>
    <url>/2024/07/01/cublas%E5%BA%93/</url>
    
    <content type="html"><![CDATA[<h2 id="cublas库"><a href="#cublas库" class="headerlink" title="cublas库"></a>cublas库</h2><h3 id="what-is-cublas？"><a href="#what-is-cublas？" class="headerlink" title="what is cublas？"></a>what is cublas？</h3><p>CUDA Basic Linear Algebra Subprograms（BLAS）提供了高效计算线性代数的方法。</p><p>有三级API和cuBLAS 扩展、辅助API：</p><p>最基础操作，例如加、减、最大值、复制、转置<br>矩阵的一般操作，例如特殊类型矩阵的乘法、rank<br>更复杂一些的例子，例如“使用一般矩阵计算批量的矩阵-矩阵乘积”，‘使用高斯复杂度降低算法计算一般矩阵的矩阵-矩阵乘积’<br>API介绍：<a href="https://docs.nvidia.com/cuda/cublas/index.html">https://docs.nvidia.com/cuda/cublas/index.html</a></p><p>样例代码：<a href="https://github.com/NVIDIA/CUDALibrarySamples/tree/master/cuBLAS">https://github.com/NVIDIA/CUDALibrarySamples/tree/master/cuBLAS</a></p><p>功能：</p><p>向量和矩阵操作：包括向量加法、向量-标量乘法、向量点积等。<br>矩阵乘法：支持各种形式的矩阵乘法，包括方阵乘法、矩阵-向量乘法等。<br>分解和求逆：例如LU分解、Cholesky分解和矩阵求逆等。<br>求解线性系统：使用不同的方法解决线性方程组。</p><h3 id="实例程序："><a href="#实例程序：" class="headerlink" title="实例程序："></a>实例程序：</h3><p>计算三角带状矩阵向量乘法</p><p><a href="https://blog.csdn.net/prinTao/article/details/135634551">【cuda】六、基础库：cuBLAS入门-CSDN博客</a></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;cstdio&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;cstdlib&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;vector&gt;</span></span><br><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;hipblas.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;hip/hip_runtime.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;hip/hip_runtime_api.h&gt;</span></span><br><br><span class="hljs-comment">//#include &lt;hipblas_utils.h&gt;</span><br><br><span class="hljs-keyword">using</span> data_type = <span class="hljs-type">double</span>; <span class="hljs-comment">//数据类型为double</span><br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">(<span class="hljs-type">int</span> argc,<span class="hljs-type">char</span> *argv[])</span></span>&#123;<br><span class="hljs-comment">//声明一个hipBLAS句柄</span><br>hipblasHandle_t hipblasH = <span class="hljs-literal">NULL</span>;<br><span class="hljs-comment">//声明一个HIP 流</span><br>hipStream_t stream = <span class="hljs-literal">NULL</span>;<br><br><span class="hljs-type">const</span> <span class="hljs-type">int</span> m = <span class="hljs-number">2</span>;<span class="hljs-comment">//矩阵A的行数</span><br><span class="hljs-type">const</span> <span class="hljs-type">int</span> n = <span class="hljs-number">2</span>;<span class="hljs-comment">//矩阵A的列数</span><br><span class="hljs-type">const</span> <span class="hljs-type">int</span> k = <span class="hljs-number">1</span>;<span class="hljs-comment">//定义超对角线元素的个数（用于三角矩阵的函数）</span><br><span class="hljs-type">const</span> <span class="hljs-type">int</span> lda = m;<span class="hljs-comment">//定义矩阵A的领先维度（leading dimension）</span><br><br><span class="hljs-comment">//初始化矩阵A和向量x</span><br><span class="hljs-comment">//std::vector 是c++标准库中的动态数组类，可以存储动态变化数量的元素</span><br><span class="hljs-type">const</span> std::vector&lt;data_type&gt; A = &#123;<span class="hljs-number">1.0</span>,<span class="hljs-number">3.0</span>,<span class="hljs-number">2.0</span>,<span class="hljs-number">4.0</span>&#125;;<br>std::vector&lt;data_type&gt; x = &#123;<span class="hljs-number">5.0</span>,<span class="hljs-number">6.0</span>&#125;;<br><span class="hljs-type">const</span> <span class="hljs-type">int</span> incx = <span class="hljs-number">1</span>;<span class="hljs-comment">//x的步长</span><br><br>data_type *d_A = <span class="hljs-literal">nullptr</span>;<span class="hljs-comment">//设备端的矩阵A</span><br>data_type *d_x = <span class="hljs-literal">nullptr</span>;<span class="hljs-comment">//设备端的向量x</span><br><br><span class="hljs-comment">//hipBLAS的相关设置</span><br>hipblasFillMode_t uplo = HIPBLAS_FILL_MODE_UPPER;<span class="hljs-comment">//使用上三角形式</span><br>hipblasOperation_t transa = HIPBLAS_OP_N;<span class="hljs-comment">//矩阵A不进行转置</span><br>hipblasDiagType_t diag = HIPBLAS_DIAG_NON_UNIT;<span class="hljs-comment">//矩阵A的对角线元素不被视为1</span><br><br><span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;A\n&quot;</span>);<br><span class="hljs-comment">//print_matrix(m,n,A.data(),lda);</span><br><span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;======\n&quot;</span>);<br><br><span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;x\n&quot;</span>);<br><span class="hljs-comment">//print_vector(x.size(),x.data());</span><br><span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;=====\n&quot;</span>);<br><br><span class="hljs-comment">//step1</span><br><span class="hljs-comment">//hipblasCreate(&amp;hipblasH);</span><br><br><br><br><br><br><br><br><span class="hljs-keyword">return</span> EXIT_SUCCESS;<br><br>&#125;<br><br></code></pre></td></tr></table></figure><h3 id="卡住了，hip和cuda-blas-库不同"><a href="#卡住了，hip和cuda-blas-库不同" class="headerlink" title="卡住了，hip和cuda  blas 库不同"></a>卡住了，hip和cuda  blas 库不同</h3><p>cuda的库#include &lt;cublas_utils.h&gt;在hip中#include &lt;hipblas_utils.h&gt;，不能调用</p><p>所以后面的一些库函数也无法调用。</p><p>cublas:</p><p>API介绍：<a href="https://docs.nvidia.com/cuda/cublas/index.html">https://docs.nvidia.com/cuda/cublas/index.html</a></p><p>样例代码：<a href="https://github.com/NVIDIA/CUDALibrarySamples/tree/master/cuBLAS">https://github.com/NVIDIA/CUDALibrarySamples/tree/master/cuBLAS</a></p><p>hipblas:</p><p><a href="https://rocmdocs.amd.com/projects/hipBLAS/en/latest/index.html">hipBLAS documentation — hipBLAS 2.1.0 Documentation (amd.com)</a></p><p>有时间再研究吧。。。。。</p><p>先用cuda试试</p><p>因为比赛是用的dcu卡，要用hip，这个方法以后再研究。</p>]]></content>
    
    
    <categories>
      
      <category>hpc</category>
      
    </categories>
    
    
    <tags>
      
      <tag>hpc</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>reduce优化(补充)</title>
    <link href="/2024/06/30/reduce%E4%BC%98%E5%8C%96(%E8%A1%A5%E5%85%85)/"/>
    <url>/2024/06/30/reduce%E4%BC%98%E5%8C%96(%E8%A1%A5%E5%85%85)/</url>
    
    <content type="html"><![CDATA[<h3 id="优化6：调整Block大小"><a href="#优化6：调整Block大小" class="headerlink" title="优化6：调整Block大小"></a>优化6：调整Block大小</h3><p>保持每个block的线程数不变，一个线程处理一个数据，两个数据，四个数据。处理多少个数据比较合适呢，这个就需要试了，获得最优的NumPerThread取值。每个线程处理数据增大一倍，同时blockPerGrid 就要减少一倍。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">reduce6</span><span class="hljs-params">(<span class="hljs-type">int</span> *d_in,<span class="hljs-type">int</span> *d_out)</span></span>&#123;<br>        __shared__ <span class="hljs-type">int</span> sdata[threadsPerBlock];<br>        <span class="hljs-comment">// 数组的全局索引也要变</span><br>        <span class="hljs-type">int</span> i = threadIdx.x + blockIdx.x * (blockDim.x * NumPerThread);<br>        <span class="hljs-type">int</span> tid = threadIdx.x;<br>        <span class="hljs-comment">// 累加一个线程处理的数据</span><br>        <span class="hljs-comment">// 一定要加上这句,确保在开始累加数据之前，它的值是 0</span><br>        sdata[tid] = <span class="hljs-number">0</span>;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j = <span class="hljs-number">0</span>;j&lt;NumPerThread;j++)&#123;<br>                sdata[tid] += d_in[i + blockDim.x * j];<br>        &#125;<br>        __syncthreads();<br><br>        <span class="hljs-keyword">if</span>(blockDim.x &gt;= <span class="hljs-number">1024</span> &amp;&amp; tid &lt; <span class="hljs-number">512</span>)&#123;<br>                sdata[tid] += sdata[tid + <span class="hljs-number">512</span>];<br>                __syncthreads();<br>        &#125;<br>        <span class="hljs-keyword">if</span>(blockDim.x &gt;= <span class="hljs-number">512</span> &amp;&amp; tid &lt; <span class="hljs-number">256</span>)&#123;<br>                sdata[tid] += sdata[tid + <span class="hljs-number">256</span>];<br>                __syncthreads();<br>        &#125;<br>        <span class="hljs-keyword">if</span>(blockDim.x &gt;= <span class="hljs-number">256</span> &amp;&amp; tid &lt; <span class="hljs-number">128</span>)&#123;<br>                sdata[tid] += sdata[tid + <span class="hljs-number">128</span>];<br>                __syncthreads();<br>        &#125;<br>        <span class="hljs-keyword">if</span>(blockDim.x &gt;= <span class="hljs-number">128</span> &amp;&amp; tid &lt; <span class="hljs-number">64</span>)&#123;<br>                sdata[tid] += sdata[tid + <span class="hljs-number">64</span>];<br>                __syncthreads();<br>        &#125;<br>        <span class="hljs-keyword">if</span>(tid &lt; <span class="hljs-number">32</span>)&#123;<br>                <span class="hljs-built_in">warpReduce</span>(sdata,tid);<br>        &#125;<br>        <span class="hljs-keyword">if</span>(tid == <span class="hljs-number">0</span>)&#123;<br>                d_out[blockIdx.x] = sdata[<span class="hljs-number">0</span>];<br>        &#125;<br>&#125;<br><br>reduce6&lt;&lt;&lt;blocksPerGrid/NumPerThread,threadsPerBlock&gt;&gt;&gt;(d_a,d_partial_c);<br></code></pre></td></tr></table></figure><h3 id="优化7：shuffle指令"><a href="#优化7：shuffle指令" class="headerlink" title="优化7：shuffle指令"></a>优化7：shuffle指令</h3><h4 id="shuffle指令"><a href="#shuffle指令" class="headerlink" title="shuffle指令"></a>shuffle指令</h4><p>NV提出了Shuffle指令，对于reduce优化有着非常好的效果。目前绝大多数访存类算子，像是softmax，batch_norm，reduce等，都是用Shuffle实现。所以，在这里谈一下这么把shuffle指令用在reduce优化上。</p><p>Shuffle指令是一组针对warp的指令。Shuffle指令最重要的特性就是<strong>warp内的寄存器可以相互访问</strong>。在没有shuffle指令的时候，各个线程在进行通信时只能通过shared memory来访问彼此的寄存器。而采用了shuffle指令之后，<strong>warp内的线程可以直接对其他线程的寄存器进行访存</strong>。通过这种方式可以减少访存的延时。除此之外，带来的最大好处就是可编程性提高了，在某些场景下，<strong>就不用shared memory</strong>了。毕竟，开发者要自己去控制 shared memory还是挺麻烦的一个事。</p><p>以后有时间再研究</p>]]></content>
    
    
    <categories>
      
      <category>hpc</category>
      
    </categories>
    
    
    <tags>
      
      <tag>hpc</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>reduce逐步优化</title>
    <link href="/2024/06/29/reduce%E4%BC%98%E5%8C%96/"/>
    <url>/2024/06/29/reduce%E4%BC%98%E5%8C%96/</url>
    
    <content type="html"><![CDATA[<h3 id="参考材料"><a href="#参考材料" class="headerlink" title="参考材料"></a>参考材料</h3><p><a href="https://zhuanlan.zhihu.com/p/426978026">深入浅出GPU优化系列：reduce优化 - 知乎 (zhihu.com)</a></p><h3 id="reduce-baseline-算法（基础规约算法）"><a href="#reduce-baseline-算法（基础规约算法）" class="headerlink" title="reduce baseline 算法（基础规约算法）"></a>reduce baseline 算法（基础规约算法）</h3><p>我们让Num_per_block与Thread_per_block一致，即一个线程处理一个数据，每个block设定为256个线程，一个block负责256个数据的reduce工作。</p><p>假设要处理32M（32*1024）个数据，那么需要的block数为 32M&#x2F;256 &#x3D; 128 个block。</p><p>tid代表每个block里面的线程号，i代表原数组的索引号，将原数组的值分配到每个shared memory 中</p><p>sdata[tid] &#x3D; d_in[i]</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;stdio.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;stdlib.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;hip/hip_runtime.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;hip/hip_runtime_api.h&gt;</span></span><br><span class="hljs-meta">#<span class="hljs-keyword">define</span> N 32768</span><br><span class="hljs-type">const</span> <span class="hljs-type">int</span> threadsPerBlock = <span class="hljs-number">256</span>;<br><span class="hljs-type">const</span> <span class="hljs-type">int</span> blocksPerGrid = <span class="hljs-number">128</span>;<br><br><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">reduce0</span><span class="hljs-params">(<span class="hljs-type">int</span> *d_in,<span class="hljs-type">int</span> *d_out)</span></span>&#123;<br>__shared__ <span class="hljs-type">int</span> sdata[threadsPerBlock];<br><br><span class="hljs-comment">//each thread loads one element form global memory to shared memory</span><br><span class="hljs-type">int</span> i = threadIdx.x + blockIdx.x * blockDim.x;<br><span class="hljs-type">int</span> tid = threadIdx.x;<br>sdata[tid] = d_in[i];<br>__syncthreads();<br><br><span class="hljs-comment">//do reduction in shared memory</span><br><span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> s = <span class="hljs-number">1</span>;s&lt;blockDim.x;s*=<span class="hljs-number">2</span>)&#123;<br><span class="hljs-keyword">if</span>(tid%(<span class="hljs-number">2</span>*s)==<span class="hljs-number">0</span>)&#123;<br>sdata[tid] += sdata[tid+s];<br>&#125;<br>__syncthreads();<br>&#125;<br><br><span class="hljs-comment">//wirte result for this block to global memory</span><br><span class="hljs-keyword">if</span>(tid ==<span class="hljs-number">0</span>)&#123;<br>d_out[blockIdx.x] = sdata[<span class="hljs-number">0</span>];<br>&#125;<br><br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">(<span class="hljs-type">void</span>)</span></span>&#123;<br><br><span class="hljs-type">int</span> a[N],partial_c[blocksPerGrid];<br><span class="hljs-type">long</span> c;<br><span class="hljs-type">int</span> *d_a,*d_partial_c;<br><span class="hljs-built_in">hipMalloc</span>((<span class="hljs-type">void</span> **)&amp;d_a,N*<span class="hljs-built_in">sizeof</span>(<span class="hljs-type">int</span>));<br><span class="hljs-built_in">hipMalloc</span>((<span class="hljs-type">void</span> **)&amp;d_partial_c,blocksPerGrid*<span class="hljs-built_in">sizeof</span>(<span class="hljs-type">int</span>));<br><span class="hljs-comment">//initial</span><br><span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i&lt;N;i++)&#123;<br>a[i] = i;<br>&#125;<br><span class="hljs-built_in">hipMemcpy</span>(d_a,a,N*<span class="hljs-built_in">sizeof</span>(<span class="hljs-type">int</span>),hipMemcpyHostToDevice);<br>reduce0&lt;&lt;&lt;blocksPerGrid,threadsPerBlock&gt;&gt;&gt;(d_a,d_partial_c);<br><span class="hljs-built_in">hipMemcpy</span>(partial_c,d_partial_c,blocksPerGrid*<span class="hljs-built_in">sizeof</span>(<span class="hljs-type">int</span>),hipMemcpyDeviceToHost);<br><br>c = <span class="hljs-number">0</span>;<br><span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i = <span class="hljs-number">0</span>;i&lt;blocksPerGrid;i++)&#123;<br>c += partial_c[i];<br>&#125;<br><br><span class="hljs-built_in">hipFree</span>(d_a);<br><span class="hljs-built_in">hipFree</span>(d_partial_c);<br><br><span class="hljs-comment">//check the result</span><br><span class="hljs-keyword">if</span>(c==<span class="hljs-number">32767</span>*<span class="hljs-number">32768</span>/<span class="hljs-number">2</span>)&#123;<br><span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;success\n&quot;</span>);<br>&#125;<span class="hljs-keyword">else</span>&#123;<br><span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;fail\n&quot;</span>);<br><span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;c=%ld&quot;</span>,c);<br>&#125;<br><br><br><span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br><br>&#125;<br><br><br></code></pre></td></tr></table></figure><h3 id="what-is-warp-and-bank"><a href="#what-is-warp-and-bank" class="headerlink" title="what is warp and bank"></a>what is warp and bank</h3><h4 id="warp"><a href="#warp" class="headerlink" title="warp:"></a>warp:</h4><p>并行计算时最小的并发结构，通常由连续32个thread组成,也称线程束。</p><h4 id="bank"><a href="#bank" class="headerlink" title="bank:"></a>bank:</h4><p>是对SM中共享内存的划分，划分个数与对应硬件warp中所含thread数一致。对应使用的计算能力3.x版本的显卡，一个warp含有32个thread，因此划分的bank数也为32，并且每个bank的宽度大小为4bytes，对应于一个int型或float型变量。</p><h3 id="优化1：解决warp-divergence"><a href="#优化1：解决warp-divergence" class="headerlink" title="优化1：解决warp divergence"></a>优化1：解决warp divergence</h3><h4 id="warp-divergence："><a href="#warp-divergence：" class="headerlink" title="warp divergence："></a>warp divergence：</h4><p>对于同一个warp中的所有thread是完全并行的，且必须要执行相同的指令，当同一warp中的thread分配了不同的指令时，会发生warp divergence，增加了程序的运行时间。为了有效地解决上述divergence问题，在分配任务时，尽量的使用索引号连续的thread，使活跃的thread全部集中到某些warp中，避免同一warp中同时存在活跃和不活跃两种状态的thread。</p><p>对于reduce算法问题，如果存在if-else这样的分支情况的话，thread会执行所有的分支。只是不满足条件的分支，所产生的结果不会记录下来。可以在图中看到，在每一轮迭代中都会产生两个分支，分别是红色和橙色的分支。这严重影响了代码执行的效率。其中红色的线程是符合if条件的线程，只有他们需要干活。</p><p><img src="/../images/1719653482706.png" alt="1719653482706"></p><h4 id="解决思路"><a href="#解决思路" class="headerlink" title="解决思路"></a>解决思路</h4><p>解决的方式也比较明了，就是尽可能地让所有线程走到同一个分支里面。</p><p>虽然代码依旧存在着if语句，但是却与reduce0代码有所不同。我们继续假定block中存在256个thread，即拥有256&#x2F;32&#x3D;8个warp。由于blockDim.x &#x3D; 256。对于3号warp，index &#x3D; 2乘tid&#x3D;2乘4乘32&#x3D;256，正好到第三个warp。当进行<strong>第1次迭代</strong>时，0-3号warp的index&lt;blockDim.x， 4-7号warp的index&gt;&#x3D;blockDim.x。对于每个warp而言，都只是进入到一个分支内，所以并不会存在warp divergence的情况。当进行<strong>第2次迭代</strong>时，0、1号两个warp进入计算分支。当进行<strong>第3次迭代</strong>时，只有0号warp进入计算分支。当进行<strong>第4次迭代</strong>时，只有0号warp的前16个线程进入分支。此时开始产生warp divergence。通过这种方式，我们消除了前3次迭代的warp divergence。</p><p>这样第一轮迭代只有前3个warp里面的连续线程是忙碌的，消除了warp divergence</p><h4 id="优化代码："><a href="#优化代码：" class="headerlink" title="优化代码："></a>优化代码：</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">reduce1</span><span class="hljs-params">(<span class="hljs-type">int</span> *d_in,<span class="hljs-type">int</span> *d_out)</span></span>&#123;<br>__shared__ <span class="hljs-type">int</span> sdata[threadsPerBlock];<br><br><span class="hljs-type">int</span> i = threadIdx.x + blockIdx.x * blockDim.x;<br><span class="hljs-type">int</span> tid = threadIdx.x;<br>sdata[tid] = d_in[i];<br>__syncthreads();<br><span class="hljs-comment">//do reduction in shared memory</span><br><span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> s = <span class="hljs-number">1</span>;s&lt;blockDim.x;s*=<span class="hljs-number">2</span>)&#123;<br><span class="hljs-type">int</span> index = <span class="hljs-number">2</span>*tid*s;<br><span class="hljs-keyword">if</span>(index &lt; blockDim.x)&#123;<br>sdata[index] += sdata[index+s];<br>&#125;<br>__syncthreads();<br>&#125;<br><span class="hljs-comment">//wirte result for this block to global memory</span><br><span class="hljs-keyword">if</span>(tid == <span class="hljs-number">0</span>)&#123;<br>d_out[blockIdx.x] = sdata[<span class="hljs-number">0</span>];<br>&#125;<br><br>&#125;<br></code></pre></td></tr></table></figure><h3 id="优化2：解决bank冲突"><a href="#优化2：解决bank冲突" class="headerlink" title="优化2：解决bank冲突"></a>优化2：解决bank冲突</h3><h4 id="bank-conflict"><a href="#bank-conflict" class="headerlink" title="bank conflict:"></a>bank conflict:</h4><p> 一个SM中，共享内存会被分成多个bank，共享内存中以每4*32bytes为单位，顺序的存储在bank0~bank31中，当两个不同的thread同时访问同一bank内的值时，会发生bank conflict，也会增加程序运行的时间。</p><h4 id="解决思路-1"><a href="#解决思路-1" class="headerlink" title="解决思路"></a>解决思路</h4><p>reduce1的最大问题是<strong>bank冲突</strong>。我们把目光聚焦在这个for循环中。并且只聚焦在<strong>0号warp</strong>。在<strong>第一次迭代</strong>中，0号线程需要去load shared memory的0号地址以及1号地址的数，然后写回到0号地址。而此时，这个warp中的16号线程，需要去load shared memory中的32号地址和33号地址。可以发现，0号地址跟32号地址产生了<strong>2路的bank冲突</strong>。再往后迭代会出现更多路的bank冲突</p><p>slove：</p><p>在reduce中，解决bank冲突的方式就是把for循环逆着来。原来stride从0到256，现在stride从128到0。</p><p>把目光继续看到这个for循环中，并且只分析0号warp。0号线程需要load shared memory的0号元素以及128号元素。第2轮迭代，0号线程load 0号元素和64号元素，1号线程load 1号元素和65号元素。第3轮迭代，0号线程load 0号元素和32号元素。到了4轮迭代，0号线程load 0号元素和16号元素。15号线程load 15号元素和31号元素。那16号线程呢，16号线程啥也不干，因为s&#x3D;16，16-31号线程啥也不干，跳过去了。</p><h4 id="优化代码"><a href="#优化代码" class="headerlink" title="优化代码"></a>优化代码</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">reduce2</span><span class="hljs-params">(<span class="hljs-type">int</span> *d_in,<span class="hljs-type">int</span> *d_out)</span></span>&#123;<br>        __shared__ <span class="hljs-type">int</span> sdata[threadsPerBlock];<br><br>        <span class="hljs-type">int</span> i = threadIdx.x + blockIdx.x * blockDim.x;<br>        <span class="hljs-type">int</span> tid = threadIdx.x;<br>        sdata[tid] = d_in[i];<br>        __syncthreads();<br><br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> s = blockDim.x/<span class="hljs-number">2</span>;s&gt;<span class="hljs-number">0</span>;s/=<span class="hljs-number">2</span>)&#123;<br>                <span class="hljs-keyword">if</span>(tid&lt;s)&#123;<br>                        sdata[tid] += sdata[tid+s];<br>                &#125;<br>                __syncthreads();<br>        &#125;<br>        <span class="hljs-keyword">if</span>(tid==<span class="hljs-number">0</span>)&#123;<br>                d_out[blockIdx.x] = sdata[<span class="hljs-number">0</span>];<br>        &#125;<br><br>&#125;<br><br></code></pre></td></tr></table></figure><h3 id="优化3：解决idle线程"><a href="#优化3：解决idle线程" class="headerlink" title="优化3：解决idle线程"></a>优化3：解决idle线程</h3><p>reduce2最大的问题就是线程的浪费。可以看到我们启动了256个线程，但是在第1轮迭代时只有128个线程在干活，第2轮迭代只有64个线程在干活，每次干活的线程都会减少一半。第一轮迭代示意图如下，只有前128个线程在load数据。后128个线程啥也不干，光看着。</p><p>每一轮迭代都有一半的线程不工作，要把所有的线程利用起来。</p><p>想来想去，那这样吧，让它好歹做一次加法。除了去global memory中取数外，再做一次加法。当然为了实现这个，block数就得改一改了。Block数量减少，Num_per_block增加一倍。也就是说原来一个block只需要管256个数就行，现在得管512个数了。</p><p>每个block还是256个线程，不同的地方在于每个block里边会处理512个数据，这样一来grid中block的数量也减少了一半。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">reduce3</span><span class="hljs-params">(<span class="hljs-type">int</span> *d_in,<span class="hljs-type">int</span> *d_out)</span></span>&#123;<br>        __shared__ <span class="hljs-type">int</span> sdata[threadsPerBlock*<span class="hljs-number">2</span>];<br><br>        <span class="hljs-type">int</span> i = threadIdx.x + blockIdx.x * (blockDim.x*<span class="hljs-number">2</span>);<br>        <span class="hljs-type">int</span> tid = threadIdx.x;<br>        sdata[tid] = d_in[i] + d_in[i + blockDim.x];<br>        __syncthreads();<br><br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> s = blockDim.x/<span class="hljs-number">2</span>;s&gt;<span class="hljs-number">0</span>;s/=<span class="hljs-number">2</span>)&#123;<br>                <span class="hljs-keyword">if</span>(tid&lt;s)&#123;<br>                        sdata[tid] += sdata[tid + s];<br>                &#125;<br>                __syncthreads();<br>        &#125;<br>        <span class="hljs-keyword">if</span>(tid==<span class="hljs-number">0</span>)&#123;<br>                d_out[blockIdx.x] = sdata[<span class="hljs-number">0</span>];<br>        &#125;<br><br>&#125;<br><br>reduce3&lt;&lt;&lt;blocksPerGrid/<span class="hljs-number">2</span>,threadsPerBlock&gt;&gt;&gt;(d_a,d_partial_c);<br></code></pre></td></tr></table></figure><h3 id="优化4：展开最后一维减少同步"><a href="#优化4：展开最后一维减少同步" class="headerlink" title="优化4：展开最后一维减少同步"></a>优化4：展开最后一维减少同步</h3><p>线程每一次迭代都减半。由于一个warp中的32个线程其实是在一个SIMD单元上，这32个线程每次都是执行同一条指令，这天然地保持了同步状态。当线程束&gt;32时需要同步，反之线程数&lt;32时，就不需要同步了。我们需要在这个时候将syncthreads操作去掉，减少同步所造成的时间浪费。</p><p>所以我们将最后一维进行展开以减少同步。</p><p>需要注意的是<strong>这个地方的cache变量需要使用volatile来进行声明</strong>，它告诉编译器每次赋值时必须将cache[tid]的值返回到全局内存中，而不是简单的读写缓存或寄存器。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">//__global__是kernel函数，可从cpu调用,__device__是只能从一个gpu函数调用，不能从cpu调用</span><br><span class="hljs-comment">//volatile 指出变量是随时可能发生变化的,与volatile变量有关的运算,不要进行编译优化,以免出错</span><br><span class="hljs-function">__device__ <span class="hljs-type">void</span> <span class="hljs-title">warpReduce</span><span class="hljs-params">(<span class="hljs-keyword">volatile</span> <span class="hljs-type">int</span> *cache,<span class="hljs-type">int</span> tid)</span></span>&#123;<br>        cache[tid] += cache[tid+<span class="hljs-number">32</span>];<br>        cache[tid] += cache[tid+<span class="hljs-number">16</span>];<br>        cache[tid] += cache[tid+<span class="hljs-number">8</span>];<br>        cache[tid] += cache[tid+<span class="hljs-number">4</span>];<br>        cache[tid] += cache[tid+<span class="hljs-number">2</span>];<br>        cache[tid] += cache[tid+<span class="hljs-number">1</span>];<br>&#125;<br><br><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">reduce4</span><span class="hljs-params">(<span class="hljs-type">int</span> *d_in,<span class="hljs-type">int</span> *d_out)</span></span>&#123;<br>        __shared__ <span class="hljs-type">int</span> sdata[threadsPerBlock*<span class="hljs-number">2</span>];<br><br>        <span class="hljs-type">int</span> i = threadIdx.x + blockIdx.x * (blockDim.x*<span class="hljs-number">2</span>);<br>        <span class="hljs-type">int</span> tid = threadIdx.x;<br>        sdata[tid] = d_in[i] + d_in[i + blockDim.x];<br>        __syncthreads();<br><br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> s = blockDim.x/<span class="hljs-number">2</span>;s&gt;<span class="hljs-number">32</span>;s/=<span class="hljs-number">2</span>)&#123;<br>                <span class="hljs-keyword">if</span>(tid&lt;s)&#123;<br>                        sdata[tid] += sdata[tid + s];<br>                &#125;<br>                __syncthreads();<br><br>        &#125;<br>        <span class="hljs-comment">//此时s = 32,因为tid&lt;s,只有一个warp里面32个线程了</span><br>        <span class="hljs-keyword">if</span>(tid&lt;<span class="hljs-number">32</span>)&#123;<br>                <span class="hljs-built_in">warpReduce</span>(sdata,tid);<br>        &#125;<br>        <span class="hljs-keyword">if</span>(tid==<span class="hljs-number">0</span>)&#123;<br>                d_out[blockIdx.x] = sdata[<span class="hljs-number">0</span>];<br>        &#125;<br>&#125;<br><br></code></pre></td></tr></table></figure><h3 id="优化5：完全展开"><a href="#优化5：完全展开" class="headerlink" title="优化5：完全展开"></a>优化5：完全展开</h3><p>对<strong>for循环</strong>进行完全展开，通过这种方式<strong>减少了条件判断的次数</strong>，因而可以实现速度的提升。</p><p>效果一般</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">//reduce 完全展开</span><br><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">reduce5</span><span class="hljs-params">(<span class="hljs-type">int</span> *d_in,<span class="hljs-type">int</span> *d_out)</span></span>&#123;<br>        __shared__ <span class="hljs-type">int</span> sdata[threadsPerBlock*<span class="hljs-number">2</span>];<br><br>        <span class="hljs-type">int</span> i = threadIdx.x + blockIdx.x * (blockDim.x*<span class="hljs-number">2</span>);<br>        <span class="hljs-type">int</span> tid = threadIdx.x;<br>        sdata[tid] = d_in[i] + d_in[i + blockDim.x];<br>        __syncthreads();<br><br>        <span class="hljs-keyword">if</span>(blockDim.x &gt;= <span class="hljs-number">1024</span> &amp;&amp; tid &lt; <span class="hljs-number">512</span>)&#123;<br>                sdata[tid] += sdata[tid + <span class="hljs-number">512</span>];<br>                __syncthreads();<br>        &#125;<br>        <span class="hljs-keyword">if</span>(blockDim.x &gt;= <span class="hljs-number">512</span> &amp;&amp; tid &lt; <span class="hljs-number">256</span>)&#123;<br>                sdata[tid] += sdata[tid + <span class="hljs-number">256</span>];<br>                __syncthreads();<br>        &#125;<br>        <span class="hljs-keyword">if</span>(blockDim.x &gt;= <span class="hljs-number">256</span> &amp;&amp; tid &lt; <span class="hljs-number">128</span>)&#123;<br>                sdata[tid] += sdata[tid + <span class="hljs-number">128</span>];<br>                __syncthreads();<br>        &#125;<br>        <span class="hljs-keyword">if</span>(blockDim.x &gt;=<span class="hljs-number">128</span> &amp;&amp; tid &lt; <span class="hljs-number">64</span>)&#123;<br>                sdata[tid] += sdata[tid + <span class="hljs-number">64</span>];<br>                __syncthreads();<br>        &#125;<br><br>        <span class="hljs-keyword">if</span>(tid &lt; <span class="hljs-number">32</span>)&#123;<br>                <span class="hljs-built_in">warpReduce</span>(sdata,tid);<br>        &#125;<br>        <span class="hljs-keyword">if</span>(tid == <span class="hljs-number">0</span>)&#123;<br>                d_out[blockIdx.x] = sdata[<span class="hljs-number">0</span>];<br>        &#125;<br><br>&#125;<br><br></code></pre></td></tr></table></figure><h3 id="优化6：调整Block大小"><a href="#优化6：调整Block大小" class="headerlink" title="优化6：调整Block大小"></a>优化6：调整Block大小</h3><h3 id="优化7：shuffle指令"><a href="#优化7：shuffle指令" class="headerlink" title="优化7：shuffle指令"></a>优化7：shuffle指令</h3>]]></content>
    
    
    <categories>
      
      <category>hpc</category>
      
    </categories>
    
    
    <tags>
      
      <tag>hpc</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>天气晴，明天有雨</title>
    <link href="/2024/06/28/%E9%9A%8F%E7%AC%941/"/>
    <url>/2024/06/28/%E9%9A%8F%E7%AC%941/</url>
    
    <content type="html"><![CDATA[<h4 id="搭建了我的博客"><a href="#搭建了我的博客" class="headerlink" title="搭建了我的博客"></a>搭建了我的博客</h4><p>github 文章永生</p><p>明天加油吧！！！</p>]]></content>
    
    
    <categories>
      
      <category>随笔</category>
      
    </categories>
    
    
    <tags>
      
      <tag>日记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>流体力学</title>
    <link href="/2024/06/28/%E6%B5%81%E4%BD%93%E5%8A%9B%E5%AD%A6/"/>
    <url>/2024/06/28/%E6%B5%81%E4%BD%93%E5%8A%9B%E5%AD%A6/</url>
    
    <content type="html"><![CDATA[<p>学习记录</p><p>1.流体研究的前提：连续介质假设<br>2.流体力学的物理量：速度，密度，压强，温度等</p><h3 id="描述流体运动的方法："><a href="#描述流体运动的方法：" class="headerlink" title="描述流体运动的方法："></a>描述流体运动的方法：</h3><p>①拉格朗日法：以一坨流体的某个质点为例，研究这个质点的随时间的变化，即盯着质点看。<br>②欧拉法：在空间取一个范围，即控制体，控制体（固定不动），如控制体中取某位置，研究该位置所在流体的性质，也就是盯着空间看。<br>●总结：拉格朗日法研究的是一坨流体（积分）或者说某个质点（微分）；欧拉法研究的是控制体（积分）或空间点（微分）</p><p><img src="/../images/1713422480785.png" alt="1713422480785"></p><h3 id="流体力学的任务（要解决什么问题）"><a href="#流体力学的任务（要解决什么问题）" class="headerlink" title="流体力学的任务（要解决什么问题）"></a>流体力学的任务（要解决什么问题）</h3><p>例子：血栓的形成，航天器内部气体的流动</p><p>研究一定条件下，任意时刻（t），任意位置（x,y,z）各种物理量（p，ρ，T，v）的分布</p><p>p(x,y,z,t),ρ（x,y,z,t）…..解这些函数，就涉及流体力学的基本方程。</p><h3 id="流体力学的基本方程（控制方程）"><a href="#流体力学的基本方程（控制方程）" class="headerlink" title="流体力学的基本方程（控制方程）"></a>流体力学的基本方程（控制方程）</h3><p>是一个方程组。</p><p>质量守恒</p><p>动量守恒（动量有三个方向，可以列三个方程）</p><p>能量守恒</p><p>要解6个值 P,ρ，T，t，v（u,v,w）速度有三个方向，一共是6个值</p><p>一共是5个方程，所以还要加一个流体力学本身具有的性质所列出的方程。</p><p><img src="/../images/1713423733375.png" alt="1713423733375"></p><h3 id="质量守恒方程（连续方程）"><a href="#质量守恒方程（连续方程）" class="headerlink" title="质量守恒方程（连续方程）"></a>质量守恒方程（连续方程）</h3><h4 id="积分形式的质量守恒"><a href="#积分形式的质量守恒" class="headerlink" title="积分形式的质量守恒"></a>积分形式的质量守恒</h4><p><img src="/../images/1713516753303.png" alt="1713516753303"></p><p><img src="/../images/1713516816649.png" alt="1713516816649"></p><h4 id="微分形式的质量守恒方程"><a href="#微分形式的质量守恒方程" class="headerlink" title="微分形式的质量守恒方程"></a>微分形式的质量守恒方程</h4><p><img src="/../images/1713516848847.png" alt="1713516848847"></p><p><img src="/../images/1713517551714.png" alt="1713517551714"></p><h4 id="散度和梯度"><a href="#散度和梯度" class="headerlink" title="散度和梯度"></a>散度和梯度</h4><p><img src="/../images/1713517596596.png" alt="1713517596596"></p><p><img src="/../images/1713517639356.png" alt="1713517639356"></p><h4 id="两种特殊情况"><a href="#两种特殊情况" class="headerlink" title="两种特殊情况"></a>两种特殊情况</h4><p>定常流动：流动是稳定的，不随时间而改变</p><p><img src="/../images/1713518306247.png" alt="1713518306247"></p><p>不可压流动：ρ为常数</p><h5 id=""><a href="#" class="headerlink" title=""></a><img src="/../images/1713518419990.png" alt="1713518419990"></h5><h4 id="连续方程微分形式的第二种推导方法"><a href="#连续方程微分形式的第二种推导方法" class="headerlink" title="连续方程微分形式的第二种推导方法"></a>连续方程微分形式的第二种推导方法</h4><p>略</p><p>散度的物理意义：</p><p><img src="/../images/1713520658383.png" alt="1713520658383"></p><p>速度散度：单位体积单位时间的体积变化</p><p><img src="/../images/1713520754996.png" alt="1713520754996"></p><h4 id="定常准一维流动的连续方程"><a href="#定常准一维流动的连续方程" class="headerlink" title="定常准一维流动的连续方程"></a>定常准一维流动的连续方程</h4><p>ρuA&#x3D;常数</p><p><img src="/../images/1713522073052.png" alt="1713522073052"></p><h3 id="动量守恒方程"><a href="#动量守恒方程" class="headerlink" title="动量守恒方程"></a>动量守恒方程</h3><h3 id="能量守恒方程"><a href="#能量守恒方程" class="headerlink" title="能量守恒方程"></a>能量守恒方程</h3>]]></content>
    
    
    <categories>
      
      <category>cfd</category>
      
    </categories>
    
    
    <tags>
      
      <tag>cfd</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>hexo_github遇到的问题</title>
    <link href="/2024/06/28/hexo_github%E9%85%8D%E7%BD%AE/"/>
    <url>/2024/06/28/hexo_github%E9%85%8D%E7%BD%AE/</url>
    
    <content type="html"><![CDATA[<h3 id="冒号后面要加空格"><a href="#冒号后面要加空格" class="headerlink" title="冒号后面要加空格"></a>冒号后面要加空格</h3><h3 id="repo写以下格式"><a href="#repo写以下格式" class="headerlink" title="repo写以下格式"></a>repo写以下格式</h3><p><img src="/../images/1719557282413.png"></p><h3 id="master分支设置"><a href="#master分支设置" class="headerlink" title="master分支设置"></a>master分支设置</h3><p><img src="/../images/1719557364439.png"></p><p><img src="/../images/1719557376929.png"></p>]]></content>
    
    
    <categories>
      
      <category>喜欢捣鼓</category>
      
    </categories>
    
    
    <tags>
      
      <tag>hexo</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>加载图片，研究一下</title>
    <link href="/2024/06/28/%E5%8A%A0%E8%BD%BD%E5%9B%BE%E7%89%87/"/>
    <url>/2024/06/28/%E5%8A%A0%E8%BD%BD%E5%9B%BE%E7%89%87/</url>
    
    <content type="html"><![CDATA[<p><img src="/../images/a.jpeg" alt="车"></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2024/06/25/hello-world/"/>
    <url>/2024/06/25/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
